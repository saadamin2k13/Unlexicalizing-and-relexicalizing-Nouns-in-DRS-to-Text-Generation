{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "modifying placeholders according to NER model\n",
        "1. all names --> PERSON\n",
        "2. all city,state,country --> GPE \n",
        "**start from here**"
      ],
      "metadata": {
        "id": "y1RDtNsXT9Tr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using large spaCy model to extract named entities line by line, replace it with its tag and save the modified text and named entities in a new file.**"
      ],
      "metadata": {
        "id": "joaJ5O3ANqdF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the code below provides,\n",
        "1. updated text file with removed named entities with their tags.\n",
        "2. dictionary for named entities."
      ],
      "metadata": {
        "id": "WSzPFJBq2V3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg \n"
      ],
      "metadata": {
        "id": "IPwKWXusNvBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "# Open the file and read the text\n",
        "with open('text.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Define the set of entity labels to replace\n",
        "replace_labels = {'PERSON', 'GPE'}\n",
        "\n",
        "# Initialize the dictionary of named entities and their tags\n",
        "ner_dict = {}\n",
        "\n",
        "# Loop through the entities in the document\n",
        "for ent in doc.ents:\n",
        "    # Check if the entity label is in the set of labels to replace\n",
        "    if ent.label_ in replace_labels:\n",
        "        # Add the named entity and its tag to the dictionary\n",
        "        ner_dict[ent.text] = ent.label_\n",
        "\n",
        "    # Replace the entity with the named entity tag\n",
        "    if ent.label_ in replace_labels:\n",
        "        text = text.replace(ent.text, ent.label_)\n",
        "\n",
        "# Write the modified text to a file\n",
        "with open('modified_text.txt', 'w') as file:\n",
        "    file.write(text)\n",
        "\n",
        "# Write the named entity dictionary to a file\n",
        "with open('named_entities.txt', 'w') as file:\n",
        "    for key, value in ner_dict.items():\n",
        "        file.write(f\"'{key}':'{value}',\\n\")\n"
      ],
      "metadata": {
        "id": "BW0Sns7INvwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying unlexicalization on NOUNS.**"
      ],
      "metadata": {
        "id": "AJGkVts_aKzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "# Open the file and read the text\n",
        "with open('text.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Define the set of entity labels to replace\n",
        "noun_labels = {'NOUN'}\n",
        "\n",
        "# Initialize the dictionary of named entities and their tags\n",
        "ner_dict = {}\n",
        "\n",
        "# Loop through the entities in the document\n",
        "for token in doc:\n",
        "    # Check if the token is a noun\n",
        "    if token.pos_ in noun_labels:\n",
        "        # Add the noun and its tag to the dictionary\n",
        "        ner_dict[token.text] = token.pos_\n",
        "\n",
        "    # Replace the noun with the noun tag\n",
        "    if token.pos_ in noun_labels:\n",
        "        text = text.replace(token.text, token.pos_)\n",
        "\n",
        "# Write the modified text to a file\n",
        "with open('modified_text.txt', 'w') as file:\n",
        "    file.write(text)\n",
        "\n",
        "# Write the named entity dictionary to a file\n",
        "with open('named_entities.txt', 'w') as file:\n",
        "    for key, value in ner_dict.items():\n",
        "        file.write(f\"'{key}':'{value}',\\n\")\n"
      ],
      "metadata": {
        "id": "AlR7qmTsaO2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dictionary processing for DRS."
      ],
      "metadata": {
        "id": "Zw9a-vrtC7UT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "converting Brad Pitt --> Brad~Pitt"
      ],
      "metadata": {
        "id": "mHRvbxFwDio1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('named_entities.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "with open('dictionary.txt', 'w') as file:\n",
        "    for line in lines:\n",
        "        words = line.split()\n",
        "        new_line = '~'.join(words)\n",
        "        file.write(new_line + '\\n')\n"
      ],
      "metadata": {
        "id": "qJr7Sb2kC99x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In DRS, all names are in Lower Case. Converting into Lower case."
      ],
      "metadata": {
        "id": "PfTvoKXeD3nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('dictionary.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "text = text.lower()\n",
        "\n",
        "with open('dictionary.txt', 'w') as file:\n",
        "    file.write(text)\n"
      ],
      "metadata": {
        "id": "7VBaZm20D9u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "converting tag into UPPER CASE."
      ],
      "metadata": {
        "id": "cYklInnlED_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/person/PERSON/g' dictionary.txt"
      ],
      "metadata": {
        "id": "4p5P84vwEGnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/gpe/GPE/g' dictionary.txt"
      ],
      "metadata": {
        "id": "P6H9ldo-EhVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All above code is for Pre-Processing of Original Text for Unlex Named Entitties.**"
      ],
      "metadata": {
        "id": "GcYgJyRONwNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "general command is 's/placeholder-name/spaCy-teg/g' file name"
      ],
      "metadata": {
        "id": "4zl8TiSaV7cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/Country_2/GPE/g' unlex1.txt"
      ],
      "metadata": {
        "id": "SUAq7wf5T_lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Post-Processing of Named Entities**"
      ],
      "metadata": {
        "id": "2vemV1ySl5zE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# Load the English language model in spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define sentence A\n",
        "sentence_a = \"Tom and Mary buy a box from the supermarket in Boston.\"\n",
        "\n",
        "# Parse sentence A with spaCy\n",
        "doc_a = nlp(sentence_a)\n",
        "\n",
        "# Extract the named entities and nouns from sentence A\n",
        "entities = [(ent.text, ent.label_) for ent in doc_a.ents]\n",
        "nouns = [token.text for token in doc_a if token.pos_ == \"NOUN\"]\n",
        "\n",
        "# Create a pandas DataFrame to store the named entities and nouns\n",
        "df = pd.DataFrame(entities + [(noun, \"NOUN\") for noun in nouns], columns=[\"text\", \"label\"])\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJcqr8OVl83I",
        "outputId": "84b06567-965a-4dae-d6e0-e4393069647f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          text   label\n",
            "0          Tom  PERSON\n",
            "1         Mary  PERSON\n",
            "2       Boston     GPE\n",
            "3          box    NOUN\n",
            "4  supermarket    NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sentence B with placeholders for [PERSON], [NOUN], and [GPE]\n",
        "sentence_b = \"PERSON and PERSON buy a NOUN from NOUN at GPE.\"\n",
        "\n",
        "# Find the recognized entities and nouns in the DataFrame\n",
        "entities_df = df[df[\"label\"].isin([\"PERSON\", \"GPE\"])]\n",
        "nouns_df = df[df[\"label\"] == \"NOUN\"]\n",
        "\n",
        "# Replace the placeholders in sentence B with the recognized entities and nouns\n",
        "for label, group in entities_df.groupby(\"label\"):\n",
        "    if label == \"PERSON\":\n",
        "        sentence_b = sentence_b.replace(label, group[\"text\"].iloc[0], 1)  # Replace only the first occurrence\n",
        "        sentence_b = sentence_b.replace(label, group[\"text\"].iloc[1], 1)  # Replace the second occurrence\n",
        "    else:\n",
        "        sentence_b = sentence_b.replace(label, group[\"text\"].iloc[0], 1)  # Replace only the first occurrence\n",
        "for i, row in nouns_df.iterrows():\n",
        "    sentence_b = sentence_b.replace(\"NOUN\", row[\"text\"], 1)  # Replace only the first occurrence\n",
        "\n",
        "print(sentence_b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUmnqTUtFOEM",
        "outputId": "4f7841f7-b44b-4151-ab8d-b4585d1625e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tom and Mary buy a box from supermarket at Boston.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "applying above logic on text files.\n"
      ],
      "metadata": {
        "id": "DAXYsRmNGwiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# Load the English language model in Spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Read the sentences from text files\n",
        "with open(\"a.txt\", \"r\") as f:\n",
        "    sentence_a = f.read().strip()\n",
        "with open(\"b.txt\", \"r\") as f:\n",
        "    sentence_b_template = f.read().strip()\n",
        "\n",
        "# Define sentence B with placeholders for [PERSON], [NOUN], and [GPE]\n",
        "sentence_b = sentence_b_template.replace(\"[PERSON]\", \"PERSON\").replace(\"[NOUN]\", \"NOUN\").replace(\"[GPE]\", \"GPE\")\n",
        "\n",
        "# Apply Spacy to sentence A to find recognized entities and nouns\n",
        "doc = nlp(sentence_a)\n",
        "entities_df = pd.DataFrame([(ent.text, ent.label_) for ent in doc.ents], columns=[\"text\", \"label\"])\n",
        "nouns_df = pd.DataFrame([(token.text, token.pos_) for token in doc if token.pos_ == \"NOUN\"], columns=[\"text\", \"pos\"])\n",
        "\n",
        "for label, group in entities_df.groupby(\"label\"):\n",
        "    if label == \"PERSON\" and len(group) >= 2:\n",
        "        sentence_b = sentence_b.replace(label, group[\"text\"].iloc[0], 1)  # Replace only the first occurrence\n",
        "        sentence_b = sentence_b.replace(label, group[\"text\"].iloc[1], 1)  # Replace the second occurrence\n",
        "    elif len(group) >= 1:\n",
        "        sentence_b = sentence_b.replace(label, group[\"text\"].iloc[0], 1)  # Replace only the first occurrence\n",
        "for i, row in nouns_df.iterrows():\n",
        "    if len(row[\"text\"]) > 0:\n",
        "        sentence_b = sentence_b.replace(\"NOUN\", row[\"text\"], 1)  # Replace only the first occurrence\n",
        "\n",
        "# Print the final sentence B\n",
        "print(sentence_b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7evETlLG0vU",
        "outputId": "b956bcd9-3984-4464-d25d-96df416c7b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 times 5 is 15.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "both files having multiple sentences."
      ],
      "metadata": {
        "id": "MpoKIIQqHzqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# Load the English language model in Spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Read the sentences from text files\n",
        "with open(\"lex.txt\", \"r\") as f:\n",
        "    sentences_a = [line.strip() for line in f.readlines()]\n",
        "with open(\"unlex.txt\", \"r\") as f:\n",
        "    sentences_b_template = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Define sentence B template with placeholders for [PERSON], [NOUN], and [GPE]\n",
        "sentence_b_template = \" \".join(sentences_b_template)\n",
        "sentence_b_template = sentence_b_template.replace(\"[PERSON]\", \"PERSON\").replace(\"[NOUN]\", \"NOUN\").replace(\"[GPE]\", \"GPE\")\n",
        "\n",
        "# Loop through each pair of sentences and replace the placeholders in sentence B with the recognized entities and nouns\n",
        "for sentence_a, sentence_b in zip(sentences_a, sentences_b_template):\n",
        "    # Apply Spacy to sentence A to find recognized entities and nouns\n",
        "    doc = nlp(sentence_a)\n",
        "    entities_df = pd.DataFrame([(ent.text, ent.label_) for ent in doc.ents], columns=[\"text\", \"label\"])\n",
        "    nouns_df = pd.DataFrame([(token.text, token.pos_) for token in doc if token.pos_ == \"NOUN\"], columns=[\"text\", \"pos\"])\n",
        "\n",
        "    # Replace the placeholders in sentence B with the recognized entities and nouns\n",
        "    for label, group in entities_df.groupby(\"label\"):\n",
        "      if label == \"PERSON\" and len(group) >= 2:\n",
        "        sentence_b = sentence_b.replace(label, group[\"text\"].iloc[0], 1)  # Replace only the first occurrence\n",
        "        sentence_b = sentence_b.replace(label, group[\"text\"].iloc[1], 1)  # Replace the second occurrence\n",
        "      elif len(group) >= 1:\n",
        "        sentence_b = sentence_b.replace(label, group[\"text\"].iloc[0], 1)  # Replace only the first occurrence\n",
        "    for i, row in nouns_df.iterrows():\n",
        "      if len(row[\"text\"]) > 0:\n",
        "        sentence_b = sentence_b.replace(\"NOUN\", row[\"text\"], 1)  # Replace only the first occurrence\n",
        "\n",
        "    # Print the final sentence B for the current sentence A\n",
        "    print(sentence_b)\n",
        "    \n"
      ],
      "metadata": {
        "id": "VllseDbAH2qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**this code is for substituting back the Named Entities and Nouns and also my main code for back substitution.**"
      ],
      "metadata": {
        "id": "xqf-bBXmXlqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But for back-substitution, we need to do some post-processing.\n",
        "1. all Names --> PERSON\n",
        "2. all locations --> GPE\n",
        "3. all nouns --> NOUN"
      ],
      "metadata": {
        "id": "GRiN68pY1hm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**processing char.txt file.**"
      ],
      "metadata": {
        "id": "FO6wgyTDlDpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/Name_1/PERSON/g' unlex.txt"
      ],
      "metadata": {
        "id": "U4hlZ09dYLJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/Name_2/PERSON/g' unlex.txt"
      ],
      "metadata": {
        "id": "WyV_ZUO5akld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/City_1/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "3-vUTXN-amai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/City_2/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "29arSsKPapeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/State_1/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "YXH_baW-arnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/State_2/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "VcWuq7rwatws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/Country_1/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "i9rXq4DIau7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/Country_2/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "0L-Lyhtnax8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/name_1/PERSON/g' unlex.txt"
      ],
      "metadata": {
        "id": "OHlPsFXn0R5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/name_2/PERSON/g' unlex.txt"
      ],
      "metadata": {
        "id": "jK7ubu_j0R5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/city_1/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "yPLUpXL50R5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/city_2/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "LC2rXQNz0R5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/state_1/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "sFcbZzwu0R50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/state_2/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "HcnBN2pb0R50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/country_1/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "CVEsBWbU0R50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/country_2/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "FnHwLX1-0R50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/noun/NOUN/g' unlex.txt"
      ],
      "metadata": {
        "id": "T_ReLkkZazDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "processing SST based file."
      ],
      "metadata": {
        "id": "R_n_Tkkki-UH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# define the regular expression to match words starting with \"noun_\"\n",
        "regex = re.compile(r\"\\bnoun_\\w+\\b\")\n",
        "\n",
        "# open the input file\n",
        "with open(\"unlex.txt\", \"r\") as input_file:\n",
        "    # read each line of the file and apply the regex\n",
        "    for line in input_file:\n",
        "        # replace all matches with \"NOUN\"\n",
        "        output_line = regex.sub(\"NOUN\", line)\n",
        "        # print the modified line\n",
        "        print(output_line, end=\"\")\n"
      ],
      "metadata": {
        "id": "ISLdRIhgjAqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**processing word.txt files.**"
      ],
      "metadata": {
        "id": "2mgojiE6lLIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/Name _ 1/PERSON/g' unlex.txt"
      ],
      "metadata": {
        "id": "_H9i61FVlTdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/Name _ 2/PERSON/g' unlex.txt"
      ],
      "metadata": {
        "id": "gVI_wr8LlTdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/City _ 1/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "BgPABDiDlTdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/City _ 2/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "L9Bts98olTdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/State _ 1/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "G-yGRE63lTdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/State _ 2/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "ksGrdqh3lTdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/Country _ 1/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "qzmjC67ClTdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/Country _ 2/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "mnXkz9LXlTdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/name _ 1/PERSON/g' unlex.txt"
      ],
      "metadata": {
        "id": "ebIhM3Et1ots"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/name _ 2/PERSON/g' unlex.txt"
      ],
      "metadata": {
        "id": "cyqlk3Kr1ot1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/city _ 1/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "n2Qe9ebt1ot2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/city _ 2/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "s011tXW81ot2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/state _ 1/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "5arrucXH1ot2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/state _ 2/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "WPW648JM1ot2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/country _ 1/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "RsZMEDQ11ot2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/country _ 2/GPE/g' unlex.txt"
      ],
      "metadata": {
        "id": "R6RC8n9Q1ot2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/noun/NOUN/g' unlex.txt"
      ],
      "metadata": {
        "id": "1z796X6ElTdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "processing SST based file."
      ],
      "metadata": {
        "id": "mkDb_9uGlTdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# define the regular expression to match words starting with \"noun_\"\n",
        "regex = re.compile(r\"\\bnoun _ \\w+\\b\")\n",
        "\n",
        "# open the input file\n",
        "with open(\"unlex.txt\", \"r\") as input_file:\n",
        "    # read each line of the file and apply the regex\n",
        "    for line in input_file:\n",
        "        # replace all matches with \"NOUN\"\n",
        "        output_line = regex.sub(\"NOUN\", line)\n",
        "        # print the modified line\n",
        "        print(output_line, end=\"\")\n"
      ],
      "metadata": {
        "id": "vj177bKhlTdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# Load the English language model in Spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Read the sentences from text files\n",
        "with open(\"lex.txt\", \"r\") as f:\n",
        "    sentences_a = [line.strip() for line in f.readlines()]\n",
        "with open(\"unlex.txt\", \"r\") as f:\n",
        "    sentences_b_template = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Define sentence B template with placeholders for [PERSON], [NOUN], and [GPE]\n",
        "sentence_b_template = \" \".join(sentences_b_template)\n",
        "sentence_b_template = sentence_b_template.replace(\"[PERSON]\", \"PERSON\").replace(\"[NOUN]\", \"NOUN\").replace(\"[GPE]\", \"GPE\")\n",
        "\n",
        "# Open a file to write the output\n",
        "with open(\"re_lex.txt\", \"w\") as f:\n",
        "    # Loop through each pair of sentences and replace the placeholders in sentence B with the recognized entities and nouns\n",
        "    for sentence_a, sentence_b in zip(sentences_a, sentences_b_template):\n",
        "        # Apply Spacy to sentence A to find recognized entities and nouns\n",
        "        doc = nlp(sentence_a)\n",
        "        entities_df = pd.DataFrame([(ent.text, ent.label_) for ent in doc.ents], columns=[\"text\", \"label\"])\n",
        "        nouns_df = pd.DataFrame([(token.text, token.pos_) for token in doc if token.pos_ == \"NOUN\"], columns=[\"text\", \"pos\"])\n",
        "\n",
        "        # Replace the placeholders in sentence B with the recognized entities and nouns\n",
        "        for label, group in entities_df.groupby(\"label\"):\n",
        "          if label == \"PERSON\" and len(group) >= 2:\n",
        "            sentence_b = sentence_b.replace(label, group[\"text\"].iloc[0], 1)  # Replace only the first occurrence\n",
        "            sentence_b = sentence_b.replace(label, group[\"text\"].iloc[1], 1)  # Replace the second occurrence\n",
        "          elif len(group) >= 1:\n",
        "            sentence_b = sentence_b.replace(label, group[\"text\"].iloc[0], 1)  # Replace only the first occurrence\n",
        "        for i, row in nouns_df.iterrows():\n",
        "          if len(row[\"text\"]) > 0:\n",
        "            sentence_b = sentence_b.replace(\"NOUN\", row[\"text\"], 1)  # Replace only the first occurrence\n",
        "\n",
        "        # Write the final sentence B for the current sentence A to the file\n",
        "        f.write(sentence_b + \"\\n\")\n"
      ],
      "metadata": {
        "id": "L23QYjWxT7l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PmQN91Pw9zg",
        "outputId": "eec15f3f-eac5-406f-f70b-004aced4896d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lex.txt  re_lex.txt  \u001b[0m\u001b[01;34msample_data\u001b[0m/  unlex.txt\n"
          ]
        }
      ]
    }
  ]
}